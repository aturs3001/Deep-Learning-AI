{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üî• Advanced Deep Learning Examples\n",
       "\n",
       "Ready to explore the full power of your deep learning framework? This notebook covers:\n",
       "\n",
       "## üéØ What We'll Cover:\n",
       "- üîµ Multi-class classification\n",
       "- üü° Non-linear data (circles)\n",
       "- üìä Regression problems\n",
       "- ‚öôÔ∏è Different optimizers comparison\n",
       "- üß† Advanced architectures with regularization\n",
       "- üìà Decision boundary visualization"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üéØ Experiment 6: Custom Dataset Challenge\n",
       "\n",
       "Let's create a custom challenging dataset and see how well our framework handles it!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a custom spiral dataset - very challenging for neural networks!\n",
       "def create_spiral_data(n_samples=300, noise=0.1):\n",
       "    \"\"\"Create a two-spiral dataset\"\"\"\n",
       "    np.random.seed(42)\n",
       "    \n",
       "    # Generate spiral 1\n",
       "    theta1 = np.linspace(0, 4*np.pi, n_samples//2)\n",
       "    r1 = theta1 / (2*np.pi)\n",
       "    x1 = r1 * np.cos(theta1) + np.random.normal(0, noise, len(theta1))\n",
       "    y1 = r1 * np.sin(theta1) + np.random.normal(0, noise, len(theta1))\n",
       "    labels1 = np.zeros(len(theta1))\n",
       "    \n",
       "    # Generate spiral 2 (rotated)\n",
       "    theta2 = np.linspace(0, 4*np.pi, n_samples//2)\n",
       "    r2 = theta2 / (2*np.pi)\n",
       "    x2 = r2 * np.cos(theta2 + np.pi) + np.random.normal(0, noise, len(theta2))\n",
       "    y2 = r2 * np.sin(theta2 + np.pi) + np.random.normal(0, noise, len(theta2))\n",
       "    labels2 = np.ones(len(theta2))\n",
       "    \n",
       "    # Combine\n",
       "    X = np.array([np.concatenate([x1, x2]), np.concatenate([y1, y2])])\n",
       "    y = np.concatenate([labels1, labels2]).reshape(1, -1)\n",
       "    \n",
       "    return X, y\n",
       "\n",
       "# Create spiral dataset\n",
       "X_spiral, y_spiral = create_spiral_data(600, noise=0.05)\n",
       "\n",
       "# Visualize the spiral data\n",
       "plt.figure(figsize=(12, 10))\n",
       "\n",
       "plt.subplot(2, 2, 1)\n",
       "colors = ['red' if label == 0 else 'blue' for label in y_spiral[0, :]]\n",
       "plt.scatter(X_spiral[0, :], X_spiral[1, :], c=colors, alpha=0.7, s=20)\n",
       "plt.title('üåÄ Two-Spiral Challenge Dataset')\n",
       "plt.xlabel('Feature 1')\n",
       "plt.ylabel('Feature 2')\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.axis('equal')\n",
       "\n",
       "print(\"üåÄ Two-Spiral Dataset Created!\")\n",
       "print(\"This is one of the most challenging 2D classification problems.\")\n",
       "print(\"Let's see how our neural network handles it!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare spiral data\n",
       "X_train_spiral, X_test_spiral, y_train_spiral, y_test_spiral = train_test_split(\n",
       "    X_spiral.T, y_spiral.T, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train_spiral, X_test_spiral = X_train_spiral.T, X_test_spiral.T\n",
       "y_train_spiral, y_test_spiral = y_train_spiral.T, y_test_spiral.T\n",
       "\n",
       "# Standardize\n",
       "scaler_spiral = StandardScaler()\n",
       "X_train_spiral = scaler_spiral.fit_transform(X_train_spiral.T).T\n",
       "X_test_spiral = scaler_spiral.transform(X_test_spiral.T).T\n",
       "\n",
       "# Create a powerful model for this challenging task\n",
       "model_spiral = NeuralNetwork(loss='binary_crossentropy', optimizer='adam')\n",
       "model_spiral.add_dense(units=50, activation='relu', input_size=2)\n",
       "model_spiral.add_dropout(rate=0.2)\n",
       "model_spiral.add_dense(units=40, activation='tanh')  # Mix of activations\n",
       "model_spiral.add_dropout(rate=0.2)\n",
       "model_spiral.add_dense(units=30, activation='relu')\n",
       "model_spiral.add_dropout(rate=0.1)\n",
       "model_spiral.add_dense(units=20, activation='relu')\n",
       "model_spiral.add_dense(units=1, activation='sigmoid')\n",
       "\n",
       "print(\"üß† Spiral Challenge Network:\")\n",
       "model_spiral.summary()\n",
       "\n",
       "# Train the spiral model\n",
       "print(\"\\nüöÄ Training on spiral data (this is tough!)...\")\n",
       "history_spiral = model_spiral.fit(\n",
       "    X_train_spiral, y_train_spiral,\n",
       "    epochs=200,  # More epochs for this difficult problem\n",
       "    batch_size=16,\n",
       "    validation_data=(X_test_spiral, y_test_spiral),\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "# Evaluate spiral model\n",
       "train_loss_spiral, train_acc_spiral = model_spiral.evaluate(X_train_spiral, y_train_spiral)\n",
       "test_loss_spiral, test_acc_spiral = model_spiral.evaluate(X_test_spiral, y_test_spiral)\n",
       "\n",
       "print(f\"\\nüåÄ Spiral Challenge Results:\")\n",
       "print(f\"Training Accuracy: {train_acc_spiral:.4f}\")\n",
       "print(f\"Test Accuracy: {test_acc_spiral:.4f}\")\n",
       "\n",
       "if test_acc_spiral > 0.95:\n",
       "    print(\"üéâ AMAZING! The network mastered the spiral challenge!\")\n",
       "elif test_acc_spiral > 0.85:\n",
       "    print(\"üòä Great job! The network learned the spiral pattern well!\")\n",
       "elif test_acc_spiral > 0.7:\n",
       "    print(\"ü§î Good progress, but spirals are tough! Try more layers or epochs.\")\n",
       "else:\n",
       "    print(\"üòÖ Spirals are really challenging! This is a very hard problem.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize spiral results with decision boundary\n",
       "plt.subplot(2, 2, 2)\n",
       "plt.plot(history_spiral['loss'], label='Training Loss', linewidth=2)\n",
       "plt.plot(history_spiral['val_loss'], label='Validation Loss', linewidth=2)\n",
       "plt.title('üåÄ Spiral Training Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.subplot(2, 2, 3)\n",
       "plt.plot(history_spiral['accuracy'], label='Training Accuracy', linewidth=2)\n",
       "plt.plot(history_spiral['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
       "plt.title('üåÄ Spiral Training Accuracy')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Create decision boundary for spiral data\n",
       "plt.subplot(2, 2, 4)\n",
       "h = 0.02\n",
       "x_min, x_max = X_test_spiral[0, :].min() - 0.5, X_test_spiral[0, :].max() + 0.5\n",
       "y_min, y_max = X_test_spiral[1, :].min() - 0.5, X_test_spiral[1, :].max() + 0.5\n",
       "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
       "                     np.arange(y_min, y_max, h))\n",
       "\n",
       "mesh_points = np.c_[xx.ravel(), yy.ravel()].T\n",
       "Z = model_spiral.predict(mesh_points)\n",
       "Z = Z.reshape(xx.shape)\n",
       "\n",
       "plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
       "colors = ['red' if label == 0 else 'blue' for label in y_test_spiral[0, :]]\n",
       "plt.scatter(X_test_spiral[0, :], X_test_spiral[1, :], c=colors, edgecolors='black', s=20)\n",
       "plt.title(f'üåÄ Spiral Decision Boundary\\nAcc: {test_acc_spiral:.3f}')\n",
       "plt.xlabel('Feature 1')\n",
       "plt.ylabel('Feature 2')\n",
       "plt.colorbar(label='Prediction')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üìà Final Performance Dashboard\n",
       "\n",
       "Let's create a comprehensive summary of all our experiments!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create comprehensive performance dashboard\n",
       "plt.figure(figsize=(20, 15))\n",
       "\n",
       "# Experiment results summary\n",
       "experiments = {\n",
       "    'Multi-Class': {'accuracy': test_acc, 'type': 'Classification', 'complexity': 'Medium'},\n",
       "    'Circular Data': {'accuracy': test_acc_c, 'type': 'Classification', 'complexity': 'High'},\n",
       "    'Regression': {'accuracy': r2, 'type': 'Regression', 'complexity': 'Low'},\n",
       "    'Spiral Challenge': {'accuracy': test_acc_spiral, 'type': 'Classification', 'complexity': 'Extreme'},\n",
       "}\n",
       "\n",
       "# Performance by experiment type\n",
       "plt.subplot(2, 4, 1)\n",
       "exp_names = list(experiments.keys())\n",
       "exp_scores = [experiments[name]['accuracy'] for name in exp_names]\n",
       "exp_colors = ['purple', 'gold', 'green', 'darkred']\n",
       "\n",
       "bars = plt.bar(exp_names, exp_scores, color=exp_colors, alpha=0.8)\n",
       "plt.title('üèÜ Experiment Performance Summary')\n",
       "plt.ylabel('Performance Score')\n",
       "plt.ylim(0, 1)\n",
       "plt.xticks(rotation=45)\n",
       "plt.grid(True, alpha=0.3, axis='y')\n",
       "\n",
       "# Add score labels\n",
       "for bar, score in zip(bars, exp_scores):\n",
       "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
       "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
       "\n",
       "# Optimizer comparison pie chart\n",
       "plt.subplot(2, 4, 2)\n",
       "opt_names = list(results.keys())\n",
       "opt_accs = [results[opt]['accuracy'] for opt in opt_names]\n",
       "plt.pie(opt_accs, labels=opt_names, autopct='%1.3f', startangle=90)\n",
       "plt.title('‚öôÔ∏è Optimizer Performance')\n",
       "\n",
       "# Training curves comparison - Multi-class\n",
       "plt.subplot(2, 4, 3)\n",
       "plt.plot(history_multi['loss'], label='Loss', linewidth=2, alpha=0.7)\n",
       "plt.plot(history_multi['accuracy'], label='Accuracy', linewidth=2)\n",
       "plt.title('üåà Multi-Class Training')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Metric')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Training curves comparison - Circular\n",
       "plt.subplot(2, 4, 4)\n",
       "plt.plot(history_circles['loss'], label='Loss', linewidth=2, alpha=0.7)\n",
       "plt.plot(history_circles['accuracy'], label='Accuracy', linewidth=2)\n",
       "plt.title('üü° Circular Data Training')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Metric')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Regression performance\n",
       "plt.subplot(2, 4, 5)\n",
       "plt.plot(history_reg['loss'], linewidth=3, color='green')\n",
       "plt.title(f'üìä Regression MSE\\nR¬≤ = {r2:.3f}')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('MSE Loss')\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Advanced model performance\n",
       "plt.subplot(2, 4, 6)\n",
       "plt.plot(history_adv['accuracy'], label='Train', linewidth=2)\n",
       "plt.plot(history_adv['val_accuracy'], label='Validation', linewidth=2)\n",
       "generalization_gap = train_acc_adv - test_acc_adv\n",
       "plt.title(f'üß† Advanced Model\\nGap: {generalization_gap:.3f}')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Spiral challenge\n",
       "plt.subplot(2, 4, 7)\n",
       "plt.plot(history_spiral['accuracy'], label='Train', linewidth=2)\n",
       "plt.plot(history_spiral['val_accuracy'], label='Validation', linewidth=2)\n",
       "plt.title(f'üåÄ Spiral Challenge\\nAcc: {test_acc_spiral:.3f}')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Overall framework capabilities\n",
       "plt.subplot(2, 4, 8)\n",
       "capabilities = ['Binary\\nClassification', 'Multi-Class\\nClassification', \n",
       "               'Regression', 'Non-Linear\\nPatterns', 'Regularization', \n",
       "               'Multiple\\nOptimizers']\n",
       "scores = [0.95, test_acc, r2, test_acc_c, 0.9, max(opt_accs)]\n",
       "colors = plt.cm.viridis(np.linspace(0, 1, len(capabilities)))\n",
       "\n",
       "bars = plt.bar(range(len(capabilities)), scores, color=colors, alpha=0.8)\n",
       "plt.title('üéØ Framework Capabilities')\n",
       "plt.ylabel('Performance')\n",
       "plt.xticks(range(len(capabilities)), capabilities, rotation=45, fontsize=9)\n",
       "plt.ylim(0, 1)\n",
       "plt.grid(True, alpha=0.3, axis='y')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Print comprehensive summary\n",
       "print(\"\\n\" + \"=\"*80)\n",
       "print(\"üéâ DEEP LEARNING FRAMEWORK - ADVANCED EXPERIMENTS COMPLETE!\")\n",
       "print(\"=\"*80)\n",
       "print(f\"üåà Multi-Class Classification:     {test_acc:.4f} accuracy\")\n",
       "print(f\"üü° Non-Linear (Circular) Data:     {test_acc_c:.4f} accuracy\")\n",
       "print(f\"üìä Regression Performance:         {r2:.4f} R¬≤ score\")\n",
       "print(f\"‚öôÔ∏è Best Optimizer:                {best_optimizer.upper()} ({max(opt_accs):.4f})\")\n",
       "print(f\"üß† Advanced Architecture:          {test_acc_adv:.4f} accuracy\")\n",
       "print(f\"üåÄ Spiral Challenge:               {test_acc_spiral:.4f} accuracy\")\n",
       "print(\"=\"*80)\n",
       "print(\"\\n‚úÖ Framework Capabilities Demonstrated:\")\n",
       "print(\"   üéØ Multiple problem types (classification, regression)\")\n",
       "print(\"   üß† Complex architectures with regularization\")\n",
       "print(\"   ‚öôÔ∏è Various optimization algorithms\")\n",
       "print(\"   üìä Comprehensive evaluation and visualization\")\n",
       "print(\"   üåÄ Handling of extremely challenging datasets\")\n",
       "print(\"\\nüöÄ Your deep learning framework is ready for real-world applications!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üéì Graduation: You've Mastered Advanced Deep Learning!\n",
       "\n",
       "### üèÜ **Congratulations! You've successfully completed all advanced experiments:**\n",
       "\n",
       "#### ‚úÖ **What You've Accomplished:**\n",
       "1. **üåà Multi-Class Classification** - Learned to handle multiple categories\n",
       "2. **üü° Non-Linear Pattern Recognition** - Mastered circular and complex data\n",
       "3. **üìä Regression Modeling** - Predicted continuous values accurately\n",
       "4. **‚öôÔ∏è Optimizer Mastery** - Compared and understood different optimization strategies\n",
       "5. **üß† Advanced Architectures** - Built sophisticated networks with regularization\n",
       "6. **üåÄ Extreme Challenges** - Tackled the infamous two-spiral problem\n",
       "\n",
       "#### üéØ **Key Skills Developed:**\n",
       "- **Problem Analysis** - Choosing the right approach for different data types\n",
       "- **Architecture Design** - Building networks appropriate for task complexity\n",
       "- **Hyperparameter Tuning** - Optimizing learning rates, epochs, and batch sizes\n",
       "- **Regularization Techniques** - Preventing overfitting with dropout and batch norm\n",
       "- **Performance Evaluation** - Using appropriate metrics and visualization\n",
       "- **Debugging Skills** - Understanding when and why models succeed or fail\n",
       "\n",
       "### üöÄ **Ready for Real-World Projects:**\n",
       "\n",
       "#### üìä **Business Applications:**\n",
       "- **Customer Segmentation** - Multi-class classification\n",
       "- **Price Prediction** - Regression modeling\n",
       "- **Fraud Detection** - Binary classification with imbalanced data\n",
       "- **Recommendation Systems** - Complex pattern recognition\n",
       "\n",
       "#### üî¨ **Research Applications:**\n",
       "- **Scientific Data Analysis** - Custom architectures for domain-specific problems\n",
       "- **Image Recognition** - Transfer learning principles\n",
       "- **Time Series Forecasting** - Sequential pattern learning\n",
       "- **Natural Language Processing** - Text classification and analysis\n",
       "\n",
       "### üé® **Next Level Challenges:**\n",
       "1. **Build a CNN** for image classification\n",
       "2. **Create an RNN** for sequence modeling\n",
       "3. **Implement Attention** mechanisms\n",
       "4. **Design AutoEncoders** for unsupervised learning\n",
       "5. **Explore GANs** for generative modeling\n",
       "\n",
       "### üåü **You're Now Ready To:**\n",
       "- ‚úÖ **Tackle any ML problem** with confidence\n",
       "- ‚úÖ **Design custom architectures** for specific needs\n",
       "- ‚úÖ **Debug and optimize** neural networks effectively\n",
       "- ‚úÖ **Collaborate on ML projects** with deep understanding\n",
       "- ‚úÖ **Teach others** the fundamentals of deep learning\n",
       "\n",
       "**üéâ Welcome to the advanced deep learning community! You've earned your place here through hands-on mastery of real problems and solutions.** üß†‚ú®\n",
       "\n",
       "---\n",
       "\n",
       "*Keep experimenting, keep learning, and most importantly - keep building amazing things with AI!* üöÄ"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train regression model\n",
       "print(\"üöÄ Training regression model...\")\n",
       "history_reg = model_regression.fit(\n",
       "    X_train_r, y_train_r,\n",
       "    epochs=100,\n",
       "    batch_size=32,\n",
       "    validation_data=(X_test_r, y_test_r),\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "# Make predictions and transform back to original scale\n",
       "y_pred_r = model_regression.predict(X_test_r)\n",
       "y_pred_original = scaler_y_r.inverse_transform(y_pred_r.T).T\n",
       "y_test_original = scaler_y_r.inverse_transform(y_test_r.T).T\n",
       "X_test_original = scaler_X_r.inverse_transform(X_test_r.T).T\n",
       "\n",
       "# Calculate regression metrics\n",
       "from sklearn.metrics import mean_squared_error, r2_score\n",
       "mse = mean_squared_error(y_test_original[0, :], y_pred_original[0, :])\n",
       "r2 = r2_score(y_test_original[0, :], y_pred_original[0, :])\n",
       "\n",
       "print(f\"\\nüìä Regression Results:\")\n",
       "print(f\"Mean Squared Error: {mse:.4f}\")\n",
       "print(f\"R¬≤ Score: {r2:.4f}\")\n",
       "\n",
       "# Plot regression results\n",
       "plt.figure(figsize=(15, 5))\n",
       "\n",
       "plt.subplot(1, 3, 1)\n",
       "plt.plot(history_reg['loss'], label='Training Loss', linewidth=2)\n",
       "plt.plot(history_reg['val_loss'], label='Validation Loss', linewidth=2)\n",
       "plt.title('üìâ Regression Training Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('MSE')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.subplot(1, 3, 2)\n",
       "plt.scatter(y_test_original[0, :], y_pred_original[0, :], alpha=0.6)\n",
       "plt.plot([y_test_original.min(), y_test_original.max()], \n",
       "         [y_test_original.min(), y_test_original.max()], 'r--', linewidth=2)\n",
       "plt.title('üéØ Predictions vs Actual')\n",
       "plt.xlabel('Actual Values')\n",
       "plt.ylabel('Predicted Values')\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.subplot(1, 3, 3)\n",
       "# Sort for better line plot\n",
       "sort_idx = np.argsort(X_test_original[0, :])\n",
       "plt.scatter(X_test_original[0, :], y_test_original[0, :], alpha=0.6, label='Actual', color='blue')\n",
       "plt.plot(X_test_original[0, sort_idx], y_pred_original[0, sort_idx], 'r-', linewidth=2, label='Predicted')\n",
       "plt.title('üìä Regression Fit')\n",
       "plt.xlabel('Input Feature')\n",
       "plt.ylabel('Target Value')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ‚öôÔ∏è Experiment 4: Optimizer Comparison\n",
       "\n",
       "Let's compare different optimizers on the same problem!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a challenging dataset for optimizer comparison\n",
       "X_opt, y_opt = make_classification(\n",
       "    n_samples=400,\n",
       "    n_features=2,\n",
       "    n_redundant=0,\n",
       "    n_informative=2,\n",
       "    n_clusters_per_class=2,  # More challenging\n",
       "    random_state=42\n",
       ")\n",
       "\n",
       "y_opt = (y_opt + 1) // 2  # Convert to {0, 1}\n",
       "y_opt = y_opt.reshape(1, -1)\n",
       "X_opt = X_opt.T\n",
       "\n",
       "# Split and standardize\n",
       "X_train_opt, X_test_opt, y_train_opt, y_test_opt = train_test_split(\n",
       "    X_opt.T, y_opt.T, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train_opt, X_test_opt = X_train_opt.T, X_test_opt.T\n",
       "y_train_opt, y_test_opt = y_train_opt.T, y_test_opt.T\n",
       "\n",
       "scaler_opt = StandardScaler()\n",
       "X_train_opt = scaler_opt.fit_transform(X_train_opt.T).T\n",
       "X_test_opt = scaler_opt.transform(X_test_opt.T).T\n",
       "\n",
       "# Test different optimizers\n",
       "optimizers = ['sgd', 'adam', 'rmsprop', 'adagrad']\n",
       "results = {}\n",
       "\n",
       "plt.figure(figsize=(16, 10))\n",
       "\n",
       "for i, optimizer in enumerate(optimizers):\n",
       "    print(f\"\\nüöÄ Testing {optimizer.upper()} optimizer...\")\n",
       "    \n",
       "    # Create model with current optimizer\n",
       "    model = NeuralNetwork(loss='binary_crossentropy', optimizer=optimizer)\n",
       "    model.add_dense(units=10, activation='relu', input_size=2)\n",
       "    model.add_dense(units=8, activation='relu')\n",
       "    model.add_dense(units=1, activation='sigmoid')\n",
       "    \n",
       "    # Train\n",
       "    history = model.fit(\n",
       "        X_train_opt, y_train_opt,\n",
       "        epochs=50,\n",
       "        batch_size=16,\n",
       "        validation_data=(X_test_opt, y_test_opt),\n",
       "        verbose=False\n",
       "    )\n",
       "    \n",
       "    # Evaluate\n",
       "    _, test_acc = model.evaluate(X_test_opt, y_test_opt)\n",
       "    results[optimizer] = {'history': history, 'accuracy': test_acc}\n",
       "    \n",
       "    print(f\"Final accuracy: {test_acc:.4f}\")\n",
       "    \n",
       "    # Plot training curves\n",
       "    plt.subplot(2, 2, i+1)\n",
       "    plt.plot(history['loss'], label='Training Loss', linewidth=2)\n",
       "    plt.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
       "    plt.title(f'{optimizer.upper()} - Final Acc: {test_acc:.3f}')\n",
       "    plt.xlabel('Epoch')\n",
       "    plt.ylabel('Loss')\n",
       "    plt.legend()\n",
       "    plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Print comparison summary\n",
       "print(\"\\n\" + \"=\"*50)\n",
       "print(\"üèÜ OPTIMIZER COMPARISON RESULTS\")\n",
       "print(\"=\"*50)\n",
       "for opt, result in results.items():\n",
       "    print(f\"{opt.upper():10} - Final Accuracy: {result['accuracy']:.4f}\")\n",
       "\n",
       "best_optimizer = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
       "print(f\"\\nü•á Best optimizer: {best_optimizer.upper()} ({results[best_optimizer]['accuracy']:.4f})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Experiment 5: Advanced Architecture with Regularization\n",
       "\n",
       "Let's build a sophisticated model with all the bells and whistles!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a more challenging dataset\n",
       "X_adv, y_adv = make_classification(\n",
       "    n_samples=1000,\n",
       "    n_features=4,  # More features\n",
       "    n_redundant=1,\n",
       "    n_informative=3,\n",
       "    n_classes=2,\n",
       "    n_clusters_per_class=2,\n",
       "    random_state=42\n",
       ")\n",
       "\n",
       "y_adv = (y_adv + 1) // 2\n",
       "y_adv = y_adv.reshape(1, -1)\n",
       "X_adv = X_adv.T\n",
       "\n",
       "# Split and standardize\n",
       "X_train_adv, X_test_adv, y_train_adv, y_test_adv = train_test_split(\n",
       "    X_adv.T, y_adv.T, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train_adv, X_test_adv = X_train_adv.T, X_test_adv.T\n",
       "y_train_adv, y_test_adv = y_train_adv.T, y_test_adv.T\n",
       "\n",
       "scaler_adv = StandardScaler()\n",
       "X_train_adv = scaler_adv.fit_transform(X_train_adv.T).T\n",
       "X_test_adv = scaler_adv.transform(X_test_adv.T).T\n",
       "\n",
       "print(f\"üìä Advanced dataset: {X_adv.shape[0]} features, {X_adv.shape[1]} samples\")\n",
       "\n",
       "# Build advanced architecture\n",
       "model_advanced = NeuralNetwork(loss='binary_crossentropy', optimizer='adam')\n",
       "model_advanced.add_dense(units=32, activation='relu', input_size=4)\n",
       "model_advanced.add_batch_norm()  # Batch normalization\n",
       "model_advanced.add_dropout(rate=0.3)\n",
       "model_advanced.add_dense(units=24, activation='relu')\n",
       "model_advanced.add_dropout(rate=0.3)\n",
       "model_advanced.add_dense(units=16, activation='relu')\n",
       "model_advanced.add_batch_norm()\n",
       "model_advanced.add_dropout(rate=0.2)\n",
       "model_advanced.add_dense(units=8, activation='relu')\n",
       "model_advanced.add_dense(units=1, activation='sigmoid')\n",
       "\n",
       "print(\"\\nüß† Advanced Neural Network Architecture:\")\n",
       "model_advanced.summary()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train advanced model\n",
       "print(\"üöÄ Training advanced model with regularization...\")\n",
       "history_adv = model_advanced.fit(\n",
       "    X_train_adv, y_train_adv,\n",
       "    epochs=120,\n",
       "    batch_size=32,\n",
       "    validation_data=(X_test_adv, y_test_adv),\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "# Evaluate advanced model\n",
       "train_loss_adv, train_acc_adv = model_advanced.evaluate(X_train_adv, y_train_adv)\n",
       "test_loss_adv, test_acc_adv = model_advanced.evaluate(X_test_adv, y_test_adv)\n",
       "\n",
       "print(f\"\\nüß† Advanced Model Results:\")\n",
       "print(f\"Training Accuracy: {train_acc_adv:.4f}\")\n",
       "print(f\"Test Accuracy: {test_acc_adv:.4f}\")\n",
       "print(f\"Generalization Gap: {abs(train_acc_adv - test_acc_adv):.4f}\")\n",
       "\n",
       "if abs(train_acc_adv - test_acc_adv) < 0.05:\n",
       "    print(\"‚úÖ Excellent generalization! Regularization is working well.\")\n",
       "elif abs(train_acc_adv - test_acc_adv) < 0.1:\n",
       "    print(\"üòä Good generalization with regularization.\")\n",
       "else:\n",
       "    print(\"ü§î Some overfitting detected. Consider more regularization.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üìà Final Visualization: Model Performance Summary"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create comprehensive performance summary\n",
       "plt.figure(figsize=(18, 12))\n",
       "\n",
       "# Multi-class training curves\n",
       "plt.subplot(2, 3, 1)\n",
       "plt.plot(history_multi['loss'], label='Training', linewidth=2)\n",
       "plt.plot(history_multi['val_loss'], label='Validation', linewidth=2)\n",
       "plt.title('üåà Multi-Class Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Circular data training curves\n",
       "plt.subplot(2, 3, 2)\n",
       "plt.plot(history_circles['accuracy'], label='Training', linewidth=2)\n",
       "plt.plot(history_circles['val_accuracy'], label='Validation', linewidth=2)\n",
       "plt.title('üü° Circular Data Accuracy')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Regression performance\n",
       "plt.subplot(2, 3, 3)\n",
       "plt.plot(history_reg['loss'], label='Training', linewidth=2, color='green')\n",
       "plt.plot(history_reg['val_loss'], label='Validation', linewidth=2, color='orange')\n",
       "plt.title('üìä Regression MSE')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('MSE')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Optimizer comparison bar chart\n",
       "plt.subplot(2, 3, 4)\n",
       "opts = list(results.keys())\n",
       "accs = [results[opt]['accuracy'] for opt in opts]\n",
       "colors = ['skyblue', 'lightcoral', 'lightgreen', 'plum']\n",
       "bars = plt.bar(opts, accs, color=colors)\n",
       "plt.title('‚öôÔ∏è Optimizer Comparison')\n",
       "plt.ylabel('Test Accuracy')\n",
       "plt.ylim(0, 1)\n",
       "# Add value labels on bars\n",
       "for bar, acc in zip(bars, accs):\n",
       "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
       "             f'{acc:.3f}', ha='center', va='bottom')\n",
       "plt.grid(True, alpha=0.3, axis='y')\n",
       "\n",
       "# Advanced model training curves\n",
       "plt.subplot(2, 3, 5)\n",
       "plt.plot(history_adv['loss'], label='Training', linewidth=2)\n",
       "plt.plot(history_adv['val_loss'], label='Validation', linewidth=2)\n",
       "plt.title('üß† Advanced Model Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Summary statistics\n",
       "plt.subplot(2, 3, 6)\n",
       "experiments = ['Multi-Class', 'Circular', 'Regression\\n(R¬≤)', 'Best Optimizer', 'Advanced']\n",
       "scores = [test_acc, test_acc_c, r2, max(accs), test_acc_adv]\n",
       "colors = ['purple', 'gold', 'green', 'coral', 'navy']\n",
       "bars = plt.bar(experiments, scores, color=colors)\n",
       "plt.title('üèÜ All Experiments Summary')\n",
       "plt.ylabel('Performance Score')\n",
       "plt.ylim(0, 1)\n",
       "# Add value labels\n",
       "for bar, score in zip(bars, scores):\n",
       "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
       "             f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
       "plt.xticks(rotation=45)\n",
       "plt.grid(True, alpha=0.3, axis='y')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "print(\"\\n\" + \"=\"*60)\n",
       "print(\"üéâ ADVANCED EXPERIMENTS COMPLETED!\")\n",
       "print(\"=\"*60)\n",
       "print(f\"üåà Multi-Class Classification: {test_acc:.4f} accuracy\")\n",
       "print(f\"üü° Non-Linear (Circular) Data: {test_acc_c:.4f} accuracy\")\n",
       "print(f\"üìä Regression Performance: {r2:.4f} R¬≤ score\")\n",
       "print(f\"‚öôÔ∏è Best Optimizer: {best_optimizer.upper()} ({max(accs):.4f})\")\n",
       "print(f\"üß† Advanced Architecture: {test_acc_adv:.4f} accuracy\")\n",
       "print(\"=\"*60)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üéì Key Learnings & Next Steps\n",
       "\n",
       "### üéØ What We Discovered:\n",
       "1. **Multi-class problems** require softmax activation and categorical crossentropy\n",
       "2. **Non-linear data** needs deeper networks with more neurons\n",
       "3. **Regression tasks** use linear output and MSE loss\n",
       "4. **Different optimizers** have varying convergence behaviors\n",
       "5. **Regularization** (dropout, batch norm) helps prevent overfitting\n",
       "\n",
       "### üöÄ Advanced Techniques to Try:\n",
       "- **Learning rate scheduling** - decay learning rate over time\n",
       "- **Early stopping** - stop training when validation loss stops improving\n",
       "- **Cross-validation** - more robust performance estimation\n",
       "- **Hyperparameter tuning** - optimize architecture and training params\n",
       "- **Ensemble methods** - combine multiple models\n",
       "\n",
       "### üî¨ Experiment Ideas:\n",
       "- Try different activation functions (ELU, Swish, LeakyReLU)\n",
       "- Experiment with different loss functions\n",
       "- Build deeper networks for more complex problems\n",
       "- Compare batch sizes and their effect on training\n",
       "- Test on real-world datasets\n",
       "\n",
       "**Congratulations! You've mastered the advanced features of your deep learning framework!** üéâ"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import all necessary libraries\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import sys\n",
       "import os\n",
       "from sklearn.datasets import make_classification, make_circles, make_regression\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.metrics import classification_report\n",
       "\n",
       "# Add the parent directory to path\n",
       "sys.path.insert(0, os.path.abspath('..'))\n",
       "\n",
       "from deep_learning import NeuralNetwork\n",
       "\n",
       "# Set style for better plots\n",
       "plt.style.use('default')\n",
       "plt.rcParams['figure.figsize'] = (12, 8)\n",
       "\n",
       "print(\"üöÄ Advanced Deep Learning Framework loaded!\")\n",
       "print(\"üìä Ready for advanced experiments!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üéØ Experiment 1: Multi-Class Classification\n",
       "\n",
       "Let's tackle a 3-class classification problem!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create multi-class data\n",
       "X, y = make_classification(\n",
       "    n_samples=800,\n",
       "    n_features=2,\n",
       "    n_redundant=0,\n",
       "    n_informative=2,\n",
       "    n_classes=3,\n",
       "    n_clusters_per_class=1,\n",
       "    random_state=42\n",
       ")\n",
       "\n",
       "# Convert to one-hot encoding for our framework\n",
       "n_classes = len(np.unique(y))\n",
       "y_onehot = np.zeros((n_classes, len(y)))\n",
       "for i, label in enumerate(y):\n",
       "    y_onehot[label, i] = 1\n",
       "\n",
       "X = X.T  # Shape: (n_features, n_samples)\n",
       "\n",
       "# Visualize the multi-class data\n",
       "plt.figure(figsize=(10, 8))\n",
       "colors = plt.cm.viridis(y / 2)  # 3 classes: 0, 1, 2\n",
       "plt.scatter(X[0, :], X[1, :], c=colors, alpha=0.7, edgecolors='black')\n",
       "plt.title('üåà Multi-Class Classification Dataset (3 Classes)')\n",
       "plt.xlabel('Feature 1')\n",
       "plt.ylabel('Feature 2')\n",
       "plt.colorbar(label='Class')\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.show()\n",
       "\n",
       "print(f\"üìä Data shape: X = {X.shape}, y_onehot = {y_onehot.shape}\")\n",
       "print(f\"üéØ Number of classes: {n_classes}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for training\n",
       "X_train, X_test, y_train, y_test, y_labels_train, y_labels_test = train_test_split(\n",
       "    X.T, y_onehot.T, y, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train, X_test = X_train.T, X_test.T\n",
       "y_train, y_test = y_train.T, y_test.T\n",
       "\n",
       "# Standardize\n",
       "scaler = StandardScaler()\n",
       "X_train = scaler.fit_transform(X_train.T).T\n",
       "X_test = scaler.transform(X_test.T).T\n",
       "\n",
       "# Create advanced multi-class model\n",
       "model_multiclass = NeuralNetwork(loss='categorical_crossentropy', optimizer='adam')\n",
       "model_multiclass.add_dense(units=20, activation='relu', input_size=2)\n",
       "model_multiclass.add_dense(units=15, activation='relu')\n",
       "model_multiclass.add_dropout(rate=0.2)  # Add regularization\n",
       "model_multiclass.add_dense(units=10, activation='relu')\n",
       "model_multiclass.add_dense(units=3, activation='softmax')  # 3 classes\n",
       "\n",
       "print(\"üèóÔ∏è Multi-Class Neural Network:\")\n",
       "model_multiclass.summary()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train multi-class model\n",
       "print(\"üöÄ Training multi-class classifier...\")\n",
       "history_multi = model_multiclass.fit(\n",
       "    X_train, y_train,\n",
       "    epochs=100,\n",
       "    batch_size=32,\n",
       "    validation_data=(X_test, y_test),\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "# Evaluate\n",
       "train_loss, train_acc = model_multiclass.evaluate(X_train, y_train)\n",
       "test_loss, test_acc = model_multiclass.evaluate(X_test, y_test)\n",
       "\n",
       "print(f\"\\nüìà Multi-Class Results:\")\n",
       "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
       "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
       "\n",
       "# Get detailed classification report\n",
       "y_pred = model_multiclass.predict(X_test)\n",
       "y_pred_labels = np.argmax(y_pred, axis=0)\n",
       "\n",
       "print(\"\\nüìä Detailed Classification Report:\")\n",
       "print(classification_report(y_labels_test, y_pred_labels))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üü° Experiment 2: Non-Linear Data (Circular Patterns)\n",
       "\n",
       "Can our neural network handle complex, non-linear patterns?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create circular data\n",
       "X_circles, y_circles = make_circles(n_samples=600, noise=0.1, factor=0.3, random_state=42)\n",
       "X_circles = X_circles.T\n",
       "y_circles = y_circles.reshape(1, -1)\n",
       "\n",
       "# Visualize circular data\n",
       "plt.figure(figsize=(10, 8))\n",
       "colors = ['red' if label == 0 else 'blue' for label in y_circles[0, :]]\n",
       "plt.scatter(X_circles[0, :], X_circles[1, :], c=colors, alpha=0.7, edgecolors='black')\n",
       "plt.title('üü° Circular Classification Challenge')\n",
       "plt.xlabel('Feature 1')\n",
       "plt.ylabel('Feature 2')\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.axis('equal')\n",
       "plt.show()\n",
       "\n",
       "print(\"üéØ This is a classic non-linear problem!\")\n",
       "print(\"Can our neural network separate the inner and outer circles?\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare circular data\n",
       "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
       "    X_circles.T, y_circles.T, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train_c, X_test_c = X_train_c.T, X_test_c.T\n",
       "y_train_c, y_test_c = y_train_c.T, y_test_c.T\n",
       "\n",
       "# Standardize\n",
       "scaler_c = StandardScaler()\n",
       "X_train_c = scaler_c.fit_transform(X_train_c.T).T\n",
       "X_test_c = scaler_c.transform(X_test_c.T).T\n",
       "\n",
       "# Create a more complex model for non-linear data\n",
       "model_circles = NeuralNetwork(loss='binary_crossentropy', optimizer='adam')\n",
       "model_circles.add_dense(units=25, activation='relu', input_size=2)\n",
       "model_circles.add_dropout(rate=0.2)\n",
       "model_circles.add_dense(units=20, activation='relu')\n",
       "model_circles.add_dropout(rate=0.2)\n",
       "model_circles.add_dense(units=15, activation='relu')\n",
       "model_circles.add_dense(units=1, activation='sigmoid')\n",
       "\n",
       "print(\"üèóÔ∏è Deep Neural Network for Non-Linear Data:\")\n",
       "model_circles.summary()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train the circular model\n",
       "print(\"üöÄ Training on circular data...\")\n",
       "history_circles = model_circles.fit(\n",
       "    X_train_c, y_train_c,\n",
       "    epochs=150,\n",
       "    batch_size=32,\n",
       "    validation_data=(X_test_c, y_test_c),\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "# Evaluate circular model\n",
       "train_loss_c, train_acc_c = model_circles.evaluate(X_train_c, y_train_c)\n",
       "test_loss_c, test_acc_c = model_circles.evaluate(X_test_c, y_test_c)\n",
       "\n",
       "print(f\"\\nüü° Circular Data Results:\")\n",
       "print(f\"Training Accuracy: {train_acc_c:.4f}\")\n",
       "print(f\"Test Accuracy: {test_acc_c:.4f}\")\n",
       "\n",
       "if test_acc_c > 0.95:\n",
       "    print(\"üéâ Excellent! The network learned the non-linear pattern!\")\n",
       "elif test_acc_c > 0.85:\n",
       "    print(\"üòä Good performance on this challenging dataset!\")\n",
       "else:\n",
       "    print(\"ü§î The network is struggling with the non-linear pattern. Try more layers or epochs!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üìä Experiment 3: Regression Problem\n",
       "\n",
       "Let's try a regression task - predicting continuous values!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create regression data\n",
       "X_reg, y_reg = make_regression(\n",
       "    n_samples=500,\n",
       "    n_features=1,\n",
       "    noise=10,\n",
       "    random_state=42\n",
       ")\n",
       "\n",
       "# Reshape for our framework\n",
       "X_reg = X_reg.T\n",
       "y_reg = y_reg.reshape(1, -1)\n",
       "\n",
       "# Visualize regression data\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.scatter(X_reg[0, :], y_reg[0, :], alpha=0.6, color='green')\n",
       "plt.title('üìä Regression Dataset')\n",
       "plt.xlabel('Input Feature')\n",
       "plt.ylabel('Target Value')\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.show()\n",
       "\n",
       "print(f\"üìä Regression data shape: X = {X_reg.shape}, y = {y_reg.shape}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare regression data\n",
       "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
       "    X_reg.T, y_reg.T, test_size=0.2, random_state=42\n",
       ")\n",
       "X_train_r, X_test_r = X_train_r.T, X_test_r.T\n",
       "y_train_r, y_test_r = y_train_r.T, y_test_r.T\n",
       "\n",
       "# Standardize\n",
       "scaler_X_r = StandardScaler()\n",
       "scaler_y_r = StandardScaler()\n",
       "\n",
       "X_train_r = scaler_X_r.fit_transform(X_train_r.T).T\n",
       "X_test_r = scaler_X_r.transform(X_test_r.T).T\n",
       "y_train_r = scaler_y_r.fit_transform(y_train_r.T).T\n",
       "y_test_r = scaler_y_r.transform(y_test_r.T).T\n",
       "\n",
       "# Create regression model\n",
       "model_regression = NeuralNetwork(loss='