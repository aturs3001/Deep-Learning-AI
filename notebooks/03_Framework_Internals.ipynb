{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Deep Learning Framework Internals\n",
    "\n",
    "Welcome to the framework internals! This notebook provides a deep dive into how the framework works under the hood.\n",
    "\n",
    "## üéØ What We'll Explore:\n",
    "- üèóÔ∏è **Architecture Overview** - How components work together\n",
    "- üß† **Layer Implementation** - Dense, Dropout, BatchNorm details\n",
    "- ‚ö° **Activation Functions** - Mathematical implementations\n",
    "- üöÄ **Optimizer Algorithms** - SGD, Adam, RMSprop internals\n",
    "- üìä **Loss Functions** - Forward and backward pass details\n",
    "- üîÑ **Training Loop** - Step-by-step execution\n",
    "- üõ†Ô∏è **Extensibility** - Adding custom components\n",
    "- üß™ **Testing & Validation** - Framework reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for framework analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Add the parent directory to path to import our deep learning framework\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import framework components for inspection\n",
    "from deep_learning import NeuralNetwork\n",
    "from deep_learning.layers import Dense, Dropout, BatchNormalization\n",
    "from deep_learning.activation import *\n",
    "from deep_learning.optimizers import *\n",
    "from deep_learning.loss import *\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üîß Deep Learning Framework Internals loaded!\")\n",
    "print(\"üìä Ready for detailed analysis!\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 1: Architecture Overview\n",
    "\n",
    "Let's examine the overall structure and design patterns of our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework Architecture Analysis\n",
    "def analyze_framework_structure():\n",
    "    \"\"\"Analyze the overall structure of the deep learning framework\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è DEEP LEARNING FRAMEWORK ARCHITECTURE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Core Classes Analysis\n",
    "    print(\"\\nüìã 1. CORE CLASSES:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Analyze NeuralNetwork class\n",
    "    nn_methods = [method for method in dir(NeuralNetwork) if not method.startswith('_')]\n",
    "    print(f\"üß† NeuralNetwork class:\")\n",
    "    print(f\"   Methods: {len(nn_methods)}\")\n",
    "    print(f\"   Key methods: {nn_methods[:8]}...\")\n",
    "    \n",
    "    # Analyze available layers\n",
    "    layer_classes = [Dense, Dropout, BatchNormalization]\n",
    "    print(f\"\\nüîó Layer Classes: {len(layer_classes)}\")\n",
    "    for layer_cls in layer_classes:\n",
    "        methods = [m for m in dir(layer_cls) if not m.startswith('_')]\n",
    "        print(f\"   {layer_cls.__name__}: {len(methods)} methods\")\n",
    "    \n",
    "    # 2. Activation Functions Analysis\n",
    "    print(\"\\n‚ö° 2. ACTIVATION FUNCTIONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check available activation functions\n",
    "    activation_funcs = list(ACTIVATION_FUNCTIONS.keys())\n",
    "    print(f\"Available activations: {len(activation_funcs)}\")\n",
    "    print(f\"Functions: {activation_funcs}\")\n",
    "    \n",
    "    # 3. Optimizers Analysis\n",
    "    print(\"\\nüöÄ 3. OPTIMIZERS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    optimizer_names = list(OPTIMIZERS.keys())\n",
    "    print(f\"Available optimizers: {len(optimizer_names)}\")\n",
    "    print(f\"Optimizers: {optimizer_names}\")\n",
    "    \n",
    "    # 4. Loss Functions Analysis\n",
    "    print(\"\\nüìä 4. LOSS FUNCTIONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    loss_names = list(LOSS_FUNCTIONS.keys())\n",
    "    print(f\"Available loss functions: {len(loss_names)}\")\n",
    "    print(f\"Loss functions: {loss_names}\")\n",
    "    \n",
    "    # 5. Design Patterns Analysis\n",
    "    print(\"\\nüé® 5. DESIGN PATTERNS USED:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"‚úÖ Factory Pattern: Optimizer and Loss creation\")\n",
    "    print(\"‚úÖ Strategy Pattern: Interchangeable algorithms\")\n",
    "    print(\"‚úÖ Builder Pattern: Sequential model construction\")\n",
    "    print(\"‚úÖ Template Method: Consistent layer interface\")\n",
    "    print(\"‚úÖ Registry Pattern: Component registration\")\n",
    "    \n",
    "    return {\n",
    "        'neural_network_methods': len(nn_methods),\n",
    "        'layer_classes': len(layer_classes),\n",
    "        'activation_functions': len(activation_funcs),\n",
    "        'optimizers': len(optimizer_names),\n",
    "        'loss_functions': len(loss_names)\n",
    "    }\n",
    "\n",
    "# Perform the analysis\n",
    "architecture_stats = analyze_framework_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize framework architecture\n",
    "def visualize_framework_architecture(stats):\n",
    "    \"\"\"Create visual representation of framework components\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Component Count Overview\n",
    "    components = list(stats.keys())\n",
    "    counts = list(stats.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(components)))\n",
    "    \n",
    "    ax1.bar(components, counts, color=colors, alpha=0.8)\n",
    "    ax1.set_title('üèóÔ∏è Framework Components Count', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Components')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (comp, count) in enumerate(zip(components, counts)):\n",
    "        ax1.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Framework Layer Structure (Conceptual)\n",
    "    layers = ['User Interface', 'Model Layer', 'Computation Layer', 'Math/NumPy']\n",
    "    layer_heights = [1, 1, 1, 1]\n",
    "    y_positions = [3, 2, 1, 0]\n",
    "    layer_colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "    \n",
    "    for i, (layer, height, y_pos, color) in enumerate(zip(layers, layer_heights, y_positions, layer_colors)):\n",
    "        ax2.barh(y_pos, 5, height=0.8, color=color, alpha=0.7, edgecolor='black')\n",
    "        ax2.text(2.5, y_pos, layer, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('üéØ Framework Architecture Layers', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlim(0, 5)\n",
    "    ax2.set_ylim(-0.5, 3.5)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xlabel('Abstraction Level ‚Üí')\n",
    "    \n",
    "    # 3. Data Flow Diagram\n",
    "    # Create a simple flow representation\n",
    "    flow_steps = ['Input', 'Forward Pass', 'Loss Calculation', 'Backward Pass', 'Weight Update']\n",
    "    x_positions = np.arange(len(flow_steps))\n",
    "    \n",
    "    # Draw flow boxes\n",
    "    for i, step in enumerate(flow_steps):\n",
    "        ax3.add_patch(plt.Rectangle((i-0.4, 0.4), 0.8, 0.2, \n",
    "                                   facecolor='lightsteelblue', edgecolor='navy', alpha=0.7))\n",
    "        ax3.text(i, 0.5, step, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Add arrows between steps\n",
    "        if i < len(flow_steps) - 1:\n",
    "            ax3.arrow(i+0.4, 0.5, 0.2, 0, head_width=0.05, head_length=0.05, \n",
    "                     fc='darkblue', ec='darkblue')\n",
    "    \n",
    "    ax3.set_title('üîÑ Training Data Flow', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlim(-0.5, len(flow_steps)-0.5)\n",
    "    ax3.set_ylim(0.3, 0.7)\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # 4. Component Interaction Matrix\n",
    "    components_matrix = ['NeuralNetwork', 'Layers', 'Activations', 'Optimizers', 'Loss']\n",
    "    # Create interaction matrix (simplified)\n",
    "    interaction_matrix = np.array([\n",
    "        [1, 1, 1, 1, 1],  # NeuralNetwork interacts with all\n",
    "        [1, 1, 1, 0, 0],  # Layers interact with NN and Activations\n",
    "        [1, 1, 1, 0, 0],  # Activations interact with NN and Layers\n",
    "        [1, 0, 0, 1, 0],  # Optimizers interact with NN\n",
    "        [1, 0, 0, 0, 1]   # Loss interacts with NN\n",
    "    ])\n",
    "    \n",
    "    im = ax4.imshow(interaction_matrix, cmap='RdYlBu_r', alpha=0.8)\n",
    "    ax4.set_title('üîó Component Interactions', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(range(len(components_matrix)))\n",
    "    ax4.set_yticks(range(len(components_matrix)))\n",
    "    ax4.set_xticklabels(components_matrix, rotation=45)\n",
    "    ax4.set_yticklabels(components_matrix)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(components_matrix)):\n",
    "        for j in range(len(components_matrix)):\n",
    "            text = '‚úì' if interaction_matrix[i, j] else '‚úó'\n",
    "            ax4.text(j, i, text, ha='center', va='center', \n",
    "                    color='white' if interaction_matrix[i, j] else 'black', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "visualize_framework_architecture(architecture_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Part 2: Layer Implementation Deep Dive\n",
    "\n",
    "Let's examine how different layer types are implemented and how they process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into layer implementations\n",
    "def analyze_layer_implementations():\n",
    "    \"\"\"Analyze the internal workings of different layer types\"\"\"\n",
    "    \n",
    "    print(\"üß† LAYER IMPLEMENTATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample data for testing\n",
    "    batch_size = 32\n",
    "    input_size = 10\n",
    "    output_size = 5\n",
    "    \n",
    "    # Generate sample input data\n",
    "    X_sample = np.random.randn(input_size, batch_size)\n",
    "    \n",
    "    print(f\"üìä Sample Data: {X_sample.shape} (features x samples)\")\n",
    "    \n",
    "    # 1. Dense Layer Analysis\n",
    "    print(\"\\nüîó 1. DENSE LAYER ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Create and analyze dense layer\n",
    "    dense_layer = Dense(units=output_size, activation='relu', input_size=input_size)\n",
    "    \n",
    "    print(f\"Dense Layer Configuration:\")\n",
    "    print(f\"  Input size: {input_size}\")\n",
    "    print(f\"  Output size: {output_size}\")\n",
    "    print(f\"  Weights shape: {dense_layer.weights.shape}\")\n",
    "    print(f\"  Biases shape: {dense_layer.biases.shape}\")\n",
    "    print(f\"  Total parameters: {dense_layer.weights.size + dense_layer.biases.size}\")\n",
    "    \n",
    "    # Perform forward pass\n",
    "    start_time = time.time()\n",
    "    dense_output = dense_layer.forward(X_sample)\n",
    "    forward_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Forward pass output shape: {dense_output.shape}\")\n",
    "    print(f\"  Forward pass time: {forward_time:.6f} seconds\")\n",
    "    print(f\"  Output range: [{dense_output.min():.3f}, {dense_output.max():.3f}]\")\n",
    "    \n",
    "    # 2. Dropout Layer Analysis\n",
    "    print(\"\\nüé≤ 2. DROPOUT LAYER ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    dropout_layer = Dropout(rate=0.5)\n",
    "    \n",
    "    # Test dropout in training mode\n",
    "    dropout_layer.training = True\n",
    "    dropout_output_train = dropout_layer.forward(dense_output)\n",
    "    \n",
    "    # Test dropout in inference mode\n",
    "    dropout_layer.training = False\n",
    "    dropout_output_infer = dropout_layer.forward(dense_output)\n",
    "    \n",
    "    print(f\"Dropout Configuration:\")\n",
    "    print(f\"  Rate: {dropout_layer.rate}\")\n",
    "    print(f\"  Input shape: {dense_output.shape}\")\n",
    "    print(f\"  Training output range: [{dropout_output_train.min():.3f}, {dropout_output_train.max():.3f}]\")\n",
    "    print(f\"  Inference output range: [{dropout_output_infer.min():.3f}, {dropout_output_infer.max():.3f}]\")\n",
    "    \n",
    "    # Check how many neurons were dropped\n",
    "    dropped_neurons = np.sum(dropout_output_train == 0)\n",
    "    total_neurons = dropout_output_train.size\n",
    "    actual_dropout_rate = dropped_neurons / total_neurons\n",
    "    print(f\"  Actual dropout rate: {actual_dropout_rate:.3f}\")\n",
    "    \n",
    "    # 3. Batch Normalization Analysis\n",
    "    print(\"\\nüìä 3. BATCH NORMALIZATION ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    batch_norm_layer = BatchNormalization(input_size=output_size)\n",
    "    \n",
    "    # Test with non-normalized input\n",
    "    unnormalized_input = dense_output * 10 + 5  # Scale and shift\n",
    "    batch_norm_output = batch_norm_layer.forward(unnormalized_input)\n",
    "    \n",
    "    print(f\"Batch Normalization:\")\n",
    "    print(f\"  Input mean: {unnormalized_input.mean(axis=1)[:3]}...\")\n",
    "    print(f\"  Input std: {unnormalized_input.std(axis=1)[:3]}...\")\n",
    "    print(f\"  Output mean: {batch_norm_output.mean(axis=1)[:3]}...\")\n",
    "    print(f\"  Output std: {batch_norm_output.std(axis=1)[:3]}...\")\n",
    "    print(f\"  Gamma (scale) shape: {batch_norm_layer.gamma.shape}\")\n",
    "    print(f\"  Beta (shift) shape: {batch_norm_layer.beta.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'dense_params': dense_layer.weights.size + dense_layer.biases.size,\n",
    "        'forward_time': forward_time,\n",
    "        'dropout_rate_actual': actual_dropout_rate,\n",
    "        'batch_norm_mean_reduction': unnormalized_input.mean() - batch_norm_output.mean()\n",
    "    }\n",
    "\n",
    "# Perform layer analysis\n",
    "layer_stats = analyze_layer_implementations()\n",
    "# Visualize layer behaviors\n",
    "def visualize_layer_behaviors():\n",
    "    \"\"\"Create visualizations showing how layers transform data\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Dense Layer Weight Distribution\n",
    "    dense_layer = Dense(units=50, activation='relu', input_size=20)\n",
    "    weights_flat = dense_layer.weights.flatten()\n",
    "    \n",
    "    ax1.hist(weights_flat, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('üîó Dense Layer Weight Distribution', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Weight Value')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axvline(weights_flat.mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {weights_flat.mean():.4f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Activation Function Comparisons\n",
    "    x_range = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Test different activation functions\n",
    "    activations_to_test = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
    "    colors = ['blue', 'green', 'red', 'orange']\n",
    "    \n",
    "    for activation_name, color in zip(activations_to_test, colors):\n",
    "        if activation_name in ACTIVATION_FUNCTIONS:\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]\n",
    "            y_values = activation_func(x_range.reshape(1, -1))[0]\n",
    "            ax2.plot(x_range, y_values, label=activation_name.title(), \n",
    "                    color=color, linewidth=2)\n",
    "    \n",
    "    ax2.set_title('‚ö° Activation Functions Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Input Value')\n",
    "    ax2.set_ylabel('Output Value')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(-5, 5)\n",
    "    \n",
    "    # 3. Dropout Effect Visualization\n",
    "    # Show how dropout affects different dropout rates\n",
    "    dropout_rates = [0.0, 0.2, 0.5, 0.8]\n",
    "    sample_input = np.random.randn(10, 100)  # 10 features, 100 samples\n",
    "    \n",
    "    remaining_neurons = []\n",
    "    for rate in dropout_rates:\n",
    "        dropout = Dropout(rate=rate)\n",
    "        dropout.training = True\n",
    "        output = dropout.forward(sample_input)\n",
    "        remaining = np.sum(output != 0) / output.size\n",
    "        remaining_neurons.append(remaining)\n",
    "    \n",
    "    ax3.bar(range(len(dropout_rates)), remaining_neurons, \n",
    "            color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('üé≤ Dropout Effect on Active Neurons', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Dropout Rate')\n",
    "    ax3.set_ylabel('Fraction of Active Neurons')\n",
    "    ax3.set_xticks(range(len(dropout_rates)))\n",
    "    ax3.set_xticklabels([f'{rate:.1f}' for rate in dropout_rates])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, remaining in enumerate(remaining_neurons):\n",
    "        ax3.text(i, remaining + 0.02, f'{remaining:.2f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Batch Normalization Effect\n",
    "    # Show distribution before and after batch normalization\n",
    "    batch_norm = BatchNormalization(input_size=5)\n",
    "    \n",
    "    # Create skewed input data\n",
    "    skewed_input = np.random.exponential(2, (5, 1000)) * 3 + 10\n",
    "    normalized_output = batch_norm.forward(skewed_input)\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax4.hist(skewed_input.flatten(), bins=50, alpha=0.5, \n",
    "             label='Before BatchNorm', color='red', density=True)\n",
    "    ax4.hist(normalized_output.flatten(), bins=50, alpha=0.5, \n",
    "             label='After BatchNorm', color='blue', density=True)\n",
    "    \n",
    "    ax4.set_title('üìä Batch Normalization Effect', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Value')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create layer behavior visualizations\n",
    "visualize_layer_behaviors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Part 3: Activation Function Mathematics\n",
    "\n",
    "Let's dive deep into the mathematical implementations of activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical analysis of activation functions\n",
    "def analyze_activation_mathematics():\n",
    "    \"\"\"Analyze the mathematical properties of activation functions\"\"\"\n",
    "    \n",
    "    print(\"‚ö° ACTIVATION FUNCTION MATHEMATICAL ANALYSIS\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Test range for analysis\n",
    "    x_range = np.linspace(-10, 10, 1000).reshape(1, -1)\n",
    "    \n",
    "    activation_analysis = {}\n",
    "    \n",
    "    # Analyze each activation function\n",
    "    for name, func in ACTIVATION_FUNCTIONS.items():\n",
    "        print(f\"\\nüîç {name.upper()} ANALYSIS:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            y = func(x_range)\n",
    "            \n",
    "            # Calculate properties\n",
    "            output_range = (float(np.min(y)), float(np.max(y)))\n",
    "            output_mean = float(np.mean(y))\n",
    "            output_std = float(np.std(y))\n",
    "            \n",
    "            # Check for zero-centeredness\n",
    "            zero_centered = abs(output_mean) < 0.1\n",
    "            \n",
    "            # Check for saturation (derivative close to zero)\n",
    "            # Approximate derivative using finite differences\n",
    "            dx = x_range[0, 1] - x_range[0, 0]\n",
    "            dy_approx = np.diff(y, axis=1) / dx\n",
    "            max_gradient = float(np.max(dy_approx))\n",
    "            min_gradient = float(np.min(dy_approx))\n",
    "            saturated = max(abs(max_gradient), abs(min_gradient)) < 0.01\n",
    "            \n",
    "            # Store analysis results\n",
    "            activation_analysis[name] = {\n",
    "                'output_range': output_range,\n",
    "                'output_mean': output_mean,\n",
    "                'output_std': output_std,\n",
    "                'zero_centered': zero_centered,\n",
    "                'saturated': saturated\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            for key, value in activation_analysis[name].items():\n",
    "                print(f\"{key}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name}: {e}\")\n",
    "    \n",
    "    return activation_analysis\n",
    "\n",
    "# Perform activation analysis\n",
    "activation_stats = analyze_activation_mathematics()\n",
    "\n",
    "# Visualize activation function properties\n",
    "def visualize_activation_properties(activation_stats):\n",
    "    \"\"\"Create visualizations showing activation function properties\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Output Range Comparison\n",
    "    output_ranges = [stats['output_range'] for stats in activation_stats.values()]\n",
    "    labels = list(activation_stats.keys())\n",
    "    \n",
    "    # Create a bar chart for output ranges\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.bar(labels, [r[1] - r[0] for r in output_ranges], color='skyblue')\n",
    "    ax1.set_title('üìä Output Range Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Range')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (label, r) in enumerate(zip(labels, output_ranges)):\n",
    "        ax1.text(i, r[1] + 0.01, f\"{r[1] - r[0]:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Output Mean Comparison\n",
    "    output_means = [stats['output_mean'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.bar(labels, output_means, color='lightgreen')\n",
    "    ax2.set_title('üìä Output Mean Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Mean')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, mean in enumerate(output_means):\n",
    "        ax2.text(i, mean + 0.01, f\"{mean:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Output Standard Deviation Comparison\n",
    "    output_stds = [stats['output_std'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.bar(labels, output_stds, color='salmon')\n",
    "    ax3.set_title('üìä Output Std Dev Comparison', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Standard Deviation')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, std in enumerate(output_stds):\n",
    "        ax3.text(i, std + 0.01, f\"{std:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Zero-Centeredness and Saturation\n",
    "    zero_centered = [stats['zero_centered'] for stats in activation_stats.values()]\n",
    "    saturated = [stats['saturated'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.bar(labels, zero_centered, color='lightblue', label='Zero-Centered')\n",
    "    ax4.bar(labels, saturated, color='lightcoral', alpha=0.7, label='Saturated')\n",
    "    ax4.set_title('üìä Zero-Centeredness and Saturation', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Boolean Value')\n",
    "    ax4.set_ylim(-0.1, 1.1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (zc, sat) in enumerate(zip(zero_centered, saturated)):\n",
    "        ax4.text(i, zc + 0.02, '‚úî' if zc else '‚úò', ha='center', va='bottom', color='blue')\n",
    "        ax4.text(i, sat + 0.02, '‚úî' if sat else '‚úò', ha='center', va='bottom', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create activation function property visualizations\n",
    "visualize_activation_properties(activation_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffa6bd",
   "metadata": {},
   "source": [
    "## üìä Part 4: Activation Function Properties Analysis\n",
    "\n",
    "In this section, we will analyze the properties of various activation functions used in neural networks. Activation functions play a crucial role in determining the output of each neuron and, consequently, the overall behavior of the network. Understanding their properties can help us make informed decisions when designing and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2532b4a",
   "metadata": {},
   "source": [
    "### Key Properties to Analyze\n",
    "\n",
    "1. **Output Range**: The range of values that the activation function can output.\n",
    "2. **Output Mean**: The average output value of the activation function over a range of inputs.\n",
    "3. **Output Standard Deviation**: The variability of the output values.\n",
    "4. **Zero-Centeredness**: Whether the output is centered around zero.\n",
    "5. **Saturation**: Whether the function saturates (i.e., outputs values close to the extremes of its range) for a significant portion of its input domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Activation Function Properties\n",
    "\n",
    "We will compute the above properties for a set of commonly used activation functions and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ed9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze activation function properties\n",
    "def analyze_activation_properties():\n",
    "    \"\"\"Analyze the properties of activation functions\"\"\"\n",
    "    \n",
    "    print(\"üìä ACTIVATION FUNCTION PROPERTIES ANALYSIS\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Test range for analysis\n",
    "    x_range = np.linspace(-10, 10, 1000).reshape(1, -1)\n",
    "    \n",
    "    activation_properties = {}\n",
    "    \n",
    "    # Analyze each activation function\n",
    "    for name, func in ACTIVATION_FUNCTIONS.items():\n",
    "        print(f\"\\nüîç {name.upper()} PROPERTIES:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            y = func(x_range)\n",
    "            \n",
    "            # Calculate properties\n",
    "            output_range = (float(np.min(y)), float(np.max(y)))\n",
    "            output_mean = float(np.mean(y))\n",
    "            output_std = float(np.std(y))\n",
    "            \n",
    "            # Check for zero-centeredness\n",
    "            zero_centered = abs(output_mean) < 0.1\n",
    "            \n",
    "            # Check for saturation (derivative close to zero)\n",
    "            dx = x_range[0, 1] - x_range[0, 0]\n",
    "            dy_approx = np.diff(y, axis=1) / dx\n",
    "            max_gradient = float(np.max(dy_approx))\n",
    "            min_gradient = float(np.min(dy_approx))\n",
    "            saturated = max(abs(max_gradient), abs(min_gradient)) < 0.01\n",
    "\n",
    "            \n",
    "            # Store analysis results\n",
    "            activation_properties[name] = {\n",
    "                'output_range': output_range,\n",
    "                'output_mean': output_mean,\n",
    "                'output_std': output_std,\n",
    "                'zero_centered': zero_centered,\n",
    "                'saturated': saturated\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            for key, value in activation_properties[name].items():\n",
    "                print(f\"{key}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name}: {e}\")\n",
    "    \n",
    "    return activation_properties\n",
    "\n",
    "# Perform activation properties analysis\n",
    "activation_properties_stats = analyze_activation_properties()\n",
    "\n",
    "# Visualize activation function properties\n",
    "def visualize_activation_properties(activation_stats):\n",
    "    \"\"\"Create visualizations showing activation function properties\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Output Range Comparison\n",
    "    output_ranges = [stats['output_range'] for stats in activation_stats.values()]\n",
    "    labels = list(activation_stats.keys())\n",
    "    \n",
    "    # Create a bar chart for output ranges\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.bar(labels, [r[1] - r[0] for r in output_ranges], color='skyblue')\n",
    "    ax1.set_title('üìä Output Range Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Range')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (label, r) in enumerate(zip(labels, output_ranges)):\n",
    "        ax1.text(i, r[1] + 0.01, f\"{r[1] - r[0]:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Output Mean Comparison\n",
    "    output_means = [stats['output_mean'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.bar(labels, output_means, color='lightgreen')\n",
    "    ax2.set_title('üìä Output Mean Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Mean')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, mean in enumerate(output_means):\n",
    "        ax2.text(i, mean + 0.01, f\"{mean:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Output Standard Deviation Comparison\n",
    "    output_stds = [stats['output_std'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.bar(labels, output_stds, color='salmon')\n",
    "    ax3.set_title('üìä Output Std Dev Comparison', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Standard Deviation')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, std in enumerate(output_stds):\n",
    "        ax3.text(i, std + 0.01, f\"{std:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Zero-Centeredness and Saturation\n",
    "    zero_centered = [stats['zero_centered'] for stats in activation_stats.values()]\n",
    "    saturated = [stats['saturated'] for stats in activation_stats.values()]\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.bar(labels, zero_centered, color='lightblue', label='Zero-Centered')\n",
    "    ax4.bar(labels, saturated, color='lightcoral', alpha=0.7, label='Saturated')\n",
    "    ax4.set_title('üìä Zero-Centeredness and Saturation', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Boolean Value')\n",
    "    ax4.set_ylim(-0.1, 1.1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (zc, sat) in enumerate(zip(zero_centered, saturated)):\n",
    "        ax4.text(i, zc + 0.02, '‚úî' if zc else '‚úò', ha='center', va='bottom', color='blue')\n",
    "        ax4.text(i, sat + 0.02, '‚úî' if sat else '‚úò', ha='center', va='bottom', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create activation function property visualizations\n",
    "visualize_activation_properties(activation_properties_stats)\n",
    "# Final summary of the analysis\n",
    "\n",
    "# Summarize the framework analysis results\n",
    "def summarize_framework_analysis(architecture_stats, layer_stats, activation_stats):\n",
    "    \"\"\"Summarize the analysis results in a structured format\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'architecture': architecture_stats,\n",
    "        'layer_analysis': layer_stats,\n",
    "        'activation_properties': activation_stats\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìã FRAMEWORK ANALYSIS SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"üèóÔ∏è Architecture Stats:\")\n",
    "    for key, value in summary['architecture'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nüß† Layer Analysis:\")\n",
    "    for key, value in summary['layer_analysis'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n‚ö° Activation Properties:\")\n",
    "    for key, value in summary['activation_properties'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return summary\n",
    "# Create a summary of the analysis\n",
    "framework_summary = summarize_framework_analysis(architecture_stats, layer_stats, activation_stats)\n",
    "# Save the summary to a file\n",
    "def save_summary_to_file(summary: Dict[str, Any], filename: str = 'framework_analysis_summary.txt'):\n",
    "    \"\"\"Save the analysis summary to a text file\"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"üìã FRAMEWORK ANALYSIS SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"üèóÔ∏è Architecture Stats:\\n\")\n",
    "        for key, value in summary['architecture'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\nüß† Layer Analysis:\\n\")\n",
    "        for key, value in summary['layer_analysis'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n‚ö° Activation Properties:\\n\")\n",
    "        for key, value in summary['activation_properties'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "    print(f\"üìÇ Summary saved to {filename}\"\n",
    "          # Save the summary to a file\n",
    "          )\n",
    "save_summary_to_file(framework_summary)\n",
    "# End of the analysis script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç Starting deep learning framework analysis...\")\n",
    "    analyze_framework_structure()\n",
    "    analyze_layer_implementations()\n",
    "    analyze_activation_mathematics()\n",
    "    print(\"‚úÖ Analysis complete! Check the visualizations and summary.\")\n",
    "    print(\"üìä Visualizations created and saved.\")\n",
    "    print(\"üìÇ Summary saved to 'framework_analysis_summary.txt'.\")\n",
    "    print(\"üéâ Thank you for using the Deep Learning Framework Analysis Tool!\")\n",
    "    print(\"üöÄ Ready to build and train your models with confidence!\")\n",
    "    print(\"üîó Stay tuned for more updates and features!\")\n",
    "    print(\"üëã Goodbye and happy coding!\")\n",
    "    print(\"üîö End of analysis script.\")\n",
    "    sys.exit(0)\n",
    "# End of the script"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
